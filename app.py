import streamlit as st
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.document import Document
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# -----------------------------
# 1. ì„ë² ë”© ëª¨ë¸ (CPUë¡œ ì¶©ë¶„íˆ ë¹ ë¦„)
# -----------------------------
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# -----------------------------
# 2. ê°„ë‹¨ ë¬¸ì„œ (í…ŒìŠ¤íŠ¸ìš©)
# -----------------------------
docs = [
    Document(page_content="AWS EC2ëŠ” ì•„ë§ˆì¡´ í´ë¼ìš°ë“œì˜ ê°€ìƒ ì„œë²„ì…ë‹ˆë‹¤."),
    Document(page_content="DockerëŠ” ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ê°€ìƒí™” í”Œë«í¼ì…ë‹ˆë‹¤."),
    Document(page_content="ì¿ ë²„ë„¤í‹°ìŠ¤ëŠ” ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ì…ë‹ˆë‹¤."),
]

db = FAISS.from_documents(docs, embeddings)
retriever = db.as_retriever()

# -----------------------------
# 3. GPT-OSS-20B LLM ì„¤ì •
# -----------------------------
model_id = "openai/gpt-oss-20b"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype=torch.bfloat16,  # Mac MPS GPU ì‚¬ìš© ê°€ëŠ¥
    device_map="auto"
)

# pipelineì„ ì´ìš©í•´ Harmony response format ì ìš©
hf_pipe = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256
)

llm = HuggingFacePipeline(pipeline=hf_pipe)

# -----------------------------
# 4. PromptTemplate ìƒì„±
# -----------------------------
prompt_template = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ì§€ ë§ê³ , ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ë§Œ ë‹µí•˜ì„¸ìš”.

Context:
{context}

Question: {question}
Answer:"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# -----------------------------
# 5. RetrievalQA ì²´ì¸
# -----------------------------
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt}  # ë¬¸ìì—´ ëŒ€ì‹  PromptTemplate ì‚¬ìš©
)

# -----------------------------
# 6. Streamlit UI
# -----------------------------
st.title("ğŸ” Hugging Face RAG ì±—ë´‡ (GPT-OSS-20B)")
user_q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if user_q:
    answer = qa.run(user_q)
    st.write("ğŸ¤– ë‹µë³€:", answer)
